{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio-Transfer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s73MXvxCadQT"
      },
      "source": [
        "import librosa\r\n",
        "import numpy as np\r\n",
        "import IPython.display as ipd\r\n",
        "import scipy.io.wavfile as wavfile\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHHhXhAzlt1Y"
      },
      "source": [
        "def read_as_spectrogram(filename: str, sample_size=2048, sample_rate=44100) -> np.ndarray:\n",
        "    ''' Convert audio file to a spectrogram, contained in a numpy ndarray\n",
        "    :param filename: location of audio file\n",
        "    :return: the audio file as a spectrogram with the shape (1 + sample_size/2, x.shape[0] / hop_length)\n",
        "    '''\n",
        "    # Currently defaulting sample_rate to 44100, based off intel. Librosa default is 22050.\n",
        "\n",
        "    hop_length = sample_size // 2\n",
        "\n",
        "    audio, sr = librosa.load(filename, sample_rate)\n",
        "    spectrogram = librosa.stft(audio, n_fft=sample_size, hop_length=hop_length)\n",
        "    mag_spectrogram = np.abs(spectrogram)\n",
        "    return mag_spectrogram, sr\n",
        "\n",
        "\n",
        "def spectrogram_to_wav(spectrogram: np.ndarray, output_file: str, sample_size=2048, sample_rate=44100) -> None:\n",
        "    ''' Convert a spectrogram into audio, and write it to an audio file\n",
        "    :param spectrogram: Numpy array that represents spectrogram\n",
        "    :param output_file: Path for file to write to\n",
        "    :return:\n",
        "    '''\n",
        "    # Currently defaulting sample_rate to 44100, based off intel. Librosa default is 22050.\n",
        "\n",
        "    hop_length = sample_size // 2\n",
        "    audio = librosa.griffinlim(spectrogram, hop_length=hop_length)\n",
        "    print(\"finished griffin lim\")\n",
        "    wavfile.write(output_file, sample_rate, audio)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqQ74psgagrQ"
      },
      "source": [
        "def gram_matrix(tensor):\r\n",
        "    #https://www.datacamp.com/community/tutorials/implementing-neural-style-transfer-using-tensorflow\r\n",
        "    \"\"\"\r\n",
        "    Computes the gram matrix of the input tensor, assuming it has exactly one layer.\r\n",
        "    :param input_tensor: input tensor\r\n",
        "    :return: gram matrix of the input tensor\r\n",
        "    \"\"\"\r\n",
        "    temp = tensor\r\n",
        "    temp = tf.squeeze(temp)\r\n",
        "\r\n",
        "    return tf.matmul(temp, tf.transpose(temp))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi3sdIjYahNj"
      },
      "source": [
        "def get_content_loss(mel_targets, mel_outputs):\r\n",
        "        return tf.reduce_mean(tf.losses.mean_squared_error(mel_targets, mel_outputs))\r\n",
        "        \r\n",
        "def get_style_loss(mel_targets, mel_outputs):\r\n",
        "        return tf.reduce_mean(tf.losses.mean_squared_error(gram_matrix(mel_targets), gram_matrix(mel_outputs)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_-hjVu0aulX"
      },
      "source": [
        "# I dont know if dense layers will be required have to look at the paper to see the model\r\n",
        "def model_audio_tranfer( combination_spectrogram):\r\n",
        "    # note according to keras lib sequential model is not beneficial here and vgg19 \r\n",
        "    #is for images and cannot be used for audio \r\n",
        "    # for better result add dense layers have 3 functions and name them blocks and then these block \r\n",
        "    # would have softmax and dense layers\r\n",
        "    # Flatten would be needed\r\n",
        "    _, co_time, co_frequency, co_channel = tuple(combination_spectrogram.shape)\r\n",
        "    print(combination_spectrogram.shape)\r\n",
        "    \r\n",
        "    model = tf.keras.layers.Conv2D(64, 3, activation=\"relu\",\r\n",
        "                               strides = (1, 1), input_shape=(1, co_time, co_frequency, co_channel))\r\n",
        "    # Decreased filter size b/c of mem issues.\r\n",
        "    \r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJQmKhxAau3A"
      },
      "source": [
        "def compute_loss(content_features, style_features, combination_features):\r\n",
        "\r\n",
        "    style_weight = 1e-6\r\n",
        "    content_weight = 2.5e-8\r\n",
        "\r\n",
        "    \r\n",
        "    # Add content loss\r\n",
        "\r\n",
        "    loss = content_weight * get_content_loss(content_features, combination_features)\r\n",
        "    # Add style loss\r\n",
        "\r\n",
        "    style_loss = get_style_loss(style_spectrogram, combination_spectrogram)\r\n",
        "    loss += (style_weight / len(style_features)) * style_loss\r\n",
        "\r\n",
        "    # Add total variation loss\r\n",
        "    #loss += total_variation_weight * total_variation_loss(combination_image)\r\n",
        "    return loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xSKCgJDavRj"
      },
      "source": [
        "def train_step(model,optimizer, content_features, style_features, combination_spectrogram):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        combination_features = model(combination_spectrogram)\r\n",
        "        loss = compute_loss(content_features, style_features, combination_features)\r\n",
        "        grads = tape.gradient(loss, combination_spectrogram)\r\n",
        "        # I think b/c we applied the model under the tape, and combination_features is derived from combination_spectrogram, applying \r\n",
        "        # model will be factored in for the spectrogram\r\n",
        "    optimizer.apply_gradients([(grads, combination_spectrogram)])\r\n",
        "    # TODO: The loss is not decreasing. Make sure the grads are actual numbers. If so,then the optimizer params are off?\r\n",
        "    return loss, grads"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZYmVcM5PPNE",
        "outputId": "29cde1d3-8b92-4f17-b40d-f86790c7b703"
      },
      "source": [
        "\n",
        "content_spectrogram, content_rate = read_as_spectrogram(\"content.wav\")\n",
        "print(\"original content_spectrogram shape\")\n",
        "print(content_spectrogram.shape)\n",
        "style_spectrogram, style_rate = read_as_spectrogram(\"style.wav\")\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original content_spectrogram shape\n",
            "(1025, 1293)\n",
            "finished griffin lim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKJ2V8_ya62m",
        "outputId": "3a859c49-ee53-475e-b773-eb3eb0ae9527"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(\r\n",
        "    keras.optimizers.schedules.ExponentialDecay(\r\n",
        "        initial_learning_rate=0.01, decay_steps=100, decay_rate=0.96\r\n",
        "    )\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "s_time, s_frequency = style_spectrogram.shape\r\n",
        "c_time, c_frequency = content_spectrogram.shape\r\n",
        "\r\n",
        "\r\n",
        "# change 1 to the number of trainable batches ie the first dimension\r\n",
        "content_spectrogram = tf.reshape(content_spectrogram, [1, c_time , c_frequency, 1])\r\n",
        "style_spectrogram = tf.reshape(style_spectrogram, [1, s_time, s_frequency, 1])\r\n",
        "combination_spectrogram  =   tf.Variable(tf.random.normal([1, c_time, c_frequency, 1]))\r\n",
        "model = model_audio_tranfer(combination_spectrogram)\r\n",
        "\r\n",
        "\r\n",
        "content_features = model(content_spectrogram)\r\n",
        "style_features = model(style_spectrogram)\r\n",
        "combination_features = model(combination_spectrogram)\r\n",
        "\r\n",
        "iterations = 100\r\n",
        "for i in range(iterations):\r\n",
        "    loss, grads = train_step(\r\n",
        "        model, optimizer, content_features, style_features, combination_spectrogram\r\n",
        "    )\r\n",
        "    if i == 1:\r\n",
        "      print(f\"initial loss: {loss}\")\r\n",
        "\r\n",
        "print(f\"final loss: {loss}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1025, 1293, 1)\n",
            "(1, 1025, 1293, 1)\n",
            "(1, 1023, 1291, 64)\n",
            "(1, 1023, 1291, 64)\n",
            "(1, 1023, 1291, 64)\n",
            "initial loss: 33.60617446899414\n",
            "final loss: 33.606170654296875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pA8RwGXa7L7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54abb44-c237-4233-cee2-d898746cbe6a"
      },
      "source": [
        "final_combination_spectrogram = tf.squeeze(combination_spectrogram)\r\n",
        "print('pre conversion spectrogram dims')\r\n",
        "print(final_combination_spectrogram.shape)\r\n",
        "# I'm assuming we just need to get the spectrogram back to it's original shape\r\n",
        "# its (1, x,y,1), and was originally (x,y) so just sqeeze\r\n",
        "merge_audio = spectrogram_to_wav(final_combination_spectrogram.numpy(), \"output.wav\")\r\n",
        "print(\"wrote file\")\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre conversion spectrogram dims\n",
            "(1025, 1293)\n",
            "finished griffin lim\n",
            "wrote file\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}