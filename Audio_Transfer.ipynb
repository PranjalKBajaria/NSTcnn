{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio-Transfer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s73MXvxCadQT"
      },
      "source": [
        "import librosa\r\n",
        "import numpy as np\r\n",
        "import IPython.display as ipd\r\n",
        "import scipy.io.wavfile as wavfile\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "import tensorflow as tf\r\n"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHHhXhAzlt1Y"
      },
      "source": [
        "def read_as_spectrogram(filename: str, sample_size=2048, sample_rate=44100) -> np.ndarray:\n",
        "    ''' Convert audio file to a spectrogram, contained in a numpy ndarray\n",
        "    :param filename: location of audio file\n",
        "    :return: the audio file as a spectrogram with the shape (1 + sample_size/2, x.shape[0] / hop_length)\n",
        "    '''\n",
        "    # Currently defaulting sample_rate to 44100, based off intel. Librosa default is 22050.\n",
        "\n",
        "    hop_length = sample_size // 2\n",
        "\n",
        "    audio, sr = librosa.load(filename, sample_rate)\n",
        "    spectrogram = librosa.stft(audio, n_fft=sample_size, hop_length=hop_length)\n",
        "    mag_spectrogram = np.abs(spectrogram)\n",
        "    return mag_spectrogram, sr\n",
        "\n",
        "\n",
        "def spectrogram_to_wav(spectrogram: np.ndarray, output_file: str, sample_size=2048, sample_rate=44100) -> None:\n",
        "    ''' Convert a spectrogram into audio, and write it to an audio file\n",
        "    :param spectrogram: Numpy array that represents spectrogram\n",
        "    :param output_file: Path for file to write to\n",
        "    :return:\n",
        "    '''\n",
        "    # Currently defaulting sample_rate to 44100, based off intel. Librosa default is 22050.\n",
        "\n",
        "    hop_length = sample_size // 2\n",
        "    audio = librosa.griffinlim(spectrogram, hop_length=hop_length)\n",
        "    print(\"finished griffin lim\")\n",
        "    wavfile.write(output_file, sample_rate, audio)\n"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqQ74psgagrQ"
      },
      "source": [
        "def gram_matrix(tensor):\r\n",
        "    #https://www.datacamp.com/community/tutorials/implementing-neural-style-transfer-using-tensorflow\r\n",
        "    \"\"\"\r\n",
        "    Computes the gram matrix of the input tensor, assuming it has exactly one layer.\r\n",
        "    :param input_tensor: input tensor\r\n",
        "    :return: gram matrix of the input tensor\r\n",
        "    \"\"\"\r\n",
        "    temp = tensor\r\n",
        "    temp = tf.squeeze(temp)\r\n",
        "    reshaped = tf.reshape(temp, [temp.shape[2], temp.shape[0] * temp.shape[1]])\r\n",
        "\r\n",
        "    return tf.matmul(reshaped, tf.transpose(reshaped))"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi3sdIjYahNj"
      },
      "source": [
        "def get_content_loss(targets, outputs):\r\n",
        "        return tf.reduce_mean(tf.losses.mean_squared_error(targets, outputs))\r\n",
        "        \r\n",
        "def get_style_loss(targets, outputs):\r\n",
        "        return tf.reduce_mean(tf.losses.mean_squared_error(gram_matrix(targets), gram_matrix(outputs)))"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_-hjVu0aulX"
      },
      "source": [
        "# I dont know if dense layers will be required have to look at the paper to see the model\r\n",
        "def model_audio_transfer( combination_spectrogram):\r\n",
        "    # note according to keras lib sequential model is not beneficial here and vgg19 \r\n",
        "    #is for images and cannot be used for audio \r\n",
        "    # for better result add dense layers have 3 functions and name them blocks and then these block \r\n",
        "    # would have softmax and dense layers\r\n",
        "    # Flatten would be needed\r\n",
        "    _, co_time, co_frequency, co_channel = tuple(combination_spectrogram.shape)\r\n",
        "    print(combination_spectrogram.shape)\r\n",
        "    \r\n",
        "    model = tf.keras.layers.Conv2D(64, 3, activation=\"relu\",\r\n",
        "                               strides = (1, 1), input_shape=(1, co_time, co_frequency, co_channel))\r\n",
        "    # Decreased filter size b/c of mem issues.\r\n",
        "    return model"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJQmKhxAau3A"
      },
      "source": [
        "def compute_loss(style_weight, content_weight, content_features, style_features, combination_features):\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "    \r\n",
        "    # Add content loss\r\n",
        "    # dividing by 2 b/c following formula from Zhixian paper\r\n",
        "    loss = (content_weight/2) * get_content_loss(content_features, combination_features)\r\n",
        "    # Add style loss\r\n",
        "\r\n",
        "    # Zhixian paper: N is # of feature maps, M is height * width (time * freq)\r\n",
        "    M = combination_features.shape[1] * combination_features.shape[2]\r\n",
        "    N = combination_features.shape[-1]\r\n",
        "    style_factor = (style_weight/ (4 * N** 2 * M ** 2))\r\n",
        "    style_loss = get_style_loss(style_features, combination_features)\r\n",
        "    loss += style_weight * style_loss\r\n",
        "\r\n",
        "    # Add total variation loss\r\n",
        "    #loss += total_variation_weight * total_variation_loss(combination_image)\r\n",
        "    return loss"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xSKCgJDavRj"
      },
      "source": [
        "def train_step(model,optimizer, content_features, style_features, combination_spectrogram, style_weight, content_weight):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "        combination_features = model(combination_spectrogram)\r\n",
        "        loss = compute_loss(style_weight, content_weight, content_features, style_features, combination_features)\r\n",
        "        grads = tape.gradient(loss, combination_spectrogram)\r\n",
        "    \r\n",
        "    optimizer.apply_gradients([(grads, combination_spectrogram)])\r\n",
        "    # TODO: The loss is not decreasing. Make sure the grads are actual numbers. If so,then the optimizer params are off?\r\n",
        "    return loss, grads"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZYmVcM5PPNE",
        "outputId": "5152773c-6b5c-4f1d-a94a-98ff021b2724"
      },
      "source": [
        "\n",
        "content_spectrogram, content_rate = read_as_spectrogram(\"content.wav\")\n",
        "print(\"original content_spectrogram shape\")\n",
        "print(content_spectrogram.shape)\n",
        "style_spectrogram, style_rate = read_as_spectrogram(\"style.wav\")\n",
        "\n"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original content_spectrogram shape\n",
            "(1025, 1296)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "XKJ2V8_ya62m",
        "outputId": "fcd237d6-66fe-4f62-a84d-d43feb853209"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(\r\n",
        "    learning_rate=0.6\r\n",
        ")\r\n",
        "print(f'optimizer config, {optimizer.get_config()}')\r\n",
        "style_weight = 0.001\r\n",
        "content_weight = 0.5\r\n",
        "print(f'style weight: {style_weight} content weight: {content_weight}')\r\n",
        "\r\n",
        "\r\n",
        "s_time, s_frequency = style_spectrogram.shape\r\n",
        "c_time, c_frequency = content_spectrogram.shape\r\n",
        "\r\n",
        "\r\n",
        "# change 1 to the number of trainable batches ie the first dimension\r\n",
        "content_spectrogram = tf.reshape(content_spectrogram, [1, c_time , c_frequency, 1])\r\n",
        "style_spectrogram = tf.reshape(style_spectrogram, [1, s_time, s_frequency, 1])\r\n",
        "combination_spectrogram = tf.Variable(tf.identity(content_spectrogram))\r\n",
        "\r\n",
        "model = model_audio_transfer(combination_spectrogram)\r\n",
        "\r\n",
        "\r\n",
        "content_features = model(content_spectrogram)\r\n",
        "\r\n",
        "style_features = model(style_spectrogram)\r\n",
        "print(f'style features shape: {style_features.shape}')\r\n",
        "\r\n",
        "\r\n",
        "iterations = 100\r\n",
        "print(f'num of iterations:{iterations}')\r\n",
        "loss_history = []\r\n",
        "for i in range(iterations):\r\n",
        "    loss, grads = train_step(\r\n",
        "        model, optimizer, content_features, style_features, combination_spectrogram, style_weight, content_weight\r\n",
        "    )\r\n",
        "    loss_history.append(loss)\r\n",
        "    if i == 1:\r\n",
        "      print(f\"initial loss: {loss}\")\r\n",
        "\r\n",
        "print(f\"final loss: {loss}\")\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(loss_history)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimizer config, {'name': 'SGD', 'learning_rate': 0.6, 'decay': 0.0, 'momentum': 0.0, 'nesterov': False}\n",
            "style weight: 0.001 content weight: 0.5\n",
            "(1, 1025, 1296, 1)\n",
            "style features shape: (1, 1023, 1291, 64)\n",
            "num of iterations:100\n",
            "initial loss: 10601362358272.0\n",
            "final loss: 36560876.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fef581bcb10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pA8RwGXa7L7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "d548bdc2-4df6-42d0-aac3-c91f94a4b9f6"
      },
      "source": [
        "final_combination_spectrogram = tf.squeeze(combination_spectrogram)\r\n",
        "print('pre conversion spectrogram dims')\r\n",
        "print(final_combination_spectrogram.shape)\r\n",
        "# I'm assuming we just need to get the spectrogram back to it's original shape\r\n",
        "# its (1, x,y,1), and was originally (x,y) so just sqeeze\r\n",
        "merge_audio = spectrogram_to_wav(final_combination_spectrogram.numpy(), \"output.wav\")\r\n",
        "print(\"wrote file\")\r\n",
        "\r\n"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre conversion spectrogram dims\n",
            "(1025, 1296)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ParameterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParameterError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-254-e61e2eef68a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# I'm assuming we just need to get the spectrogram back to it's original shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# its (1, x,y,1), and was originally (x,y) so just sqeeze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmerge_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectrogram_to_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_combination_spectrogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wrote file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-246-2a9a2b00f364>\u001b[0m in \u001b[0;36mspectrogram_to_wav\u001b[0;34m(spectrogram, output_file, sample_size, sample_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mhop_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgriffinlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished griffin lim\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mgriffinlim\u001b[0;34m(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, init, random_state)\u001b[0m\n\u001b[1;32m   2413\u001b[0m             \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m             \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m             \u001b[0mpad_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2416\u001b[0m         )\n\u001b[1;32m   2417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# Check audio is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Pad the time series so that frames are centered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/util/utils.py\u001b[0m in \u001b[0;36mvalid_audio\u001b[0;34m(y, mono)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Audio buffer is not finite everywhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParameterError\u001b[0m: Audio buffer is not finite everywhere"
          ]
        }
      ]
    }
  ]
}